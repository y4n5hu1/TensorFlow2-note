{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.5, shape=(), dtype=float32)\n",
      "tf.Tensor(6.5, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  metric使用\n",
    "\n",
    "metric = keras.metrics.MeanSquaredError()\n",
    "print(metric([5.], [2.]))\n",
    "print(metric([4.], [2.]))  #  metric有累加效果\n",
    "print(metric.result())\n",
    "\n",
    "metric.reset_states()  #  清除累加\n",
    "print(metric([4.], [2.]))\n",
    "print(metric.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch: 0 train mse: 1.4155911 \t vaild mse: 1.46943337  0 train mse: 1.6705226  0 train mse: 1.5592906  0 train mse: 1.4709429  0 train mse: 1.4649867  0 train mse: 1.4702524  0 train mse: 1.4696999  0 train mse: 1.4515474 1.4328837  0 train mse: 1.4301254  0 train mse: 1.4189728 \n",
      "Epoch: 1 train mse: 1.5986402 \t vaild mse: 1.43350777  1 train mse: 2.1390018  1 train mse: 1.9547393  1 train mse: 1.7923397  1 train mse: 1.7658286  1 train mse: 1.8823577  1 train mse: 2.4312477  1 train mse: 2.401092  1 train mse: 2.3141062 1 train mse: 2.1035986  1 train mse: 1.8508974   1 train mse: 1.6993598  1 train mse: 1.664032  1 train mse: 1.6033288 \n",
      "Epoch: 2 train mse: 1.344968 \t 2 train mse: 1.2096672  2 train mse: 1.361313  2 train mse: 1.3624367  2 train mse: 1.3328305 2 train mse: 1.3222331  train mse: 1.3307171  2 train mse: 1.3263547  2 train mse: 1.3216083  1.3292377  2 train mse: 1.3327167  2 train mse: 1.3286957  2 train mse: 1.3275244  2 train mse: 1.3314408 1.3360223  2 train mse: 1.3435596 1.3480054  2 train mse: 1.3416412  2 train mse: 1.338526  vaild mse: 1.4049407\n",
      "Epoch: 3 train mse: 1.31786 \t vaild mse: 1.3989354202  3 train mse: 1.2203404  3 train mse: 1.2744571  3 train mse: 1.2589662  3 train mse: 1.2895663  3 train mse: 1.3025773  train mse: 1.2877864  3 train mse: 1.2894738  3 train mse: 1.2897438   3 train mse: 1.2839609  3 train mse: 1.300768  3 train mse: 1.2907305  train mse: 1.2995192  3 train mse: 1.3068136  3 train mse: 1.3086845  3 train mse: 1.3073254  3 train mse: 1.316924  3 train mse: 1.3226845  3 train mse: 1.320795 1.3240839 \n",
      "Epoch: 4 train mse: 1.2786103 \t vaild mse: 1.39633779  1.2848841  4 train mse: 1.2434101  4 train mse: 1.2459803  4 train mse: 1.220858  4 train mse: 1.247727  1.2596024  4 train mse: 1.2846448  4 train mse: 1.284252  4 train mse: 1.2754756  4 train mse: 1.2766079  4 train mse: 1.2637833  4 train mse: 1.2705423  train mse: 1.2717172  4 train mse: 1.2739435  4 train mse: 1.2801344  4 train mse: 1.2774205  4 train mse: 1.2759933  4 train mse: 1.2764152  4 train mse: 1.2791737 \n",
      "Epoch: 5 train mse: 1.2636942 \t vaild mse: 1.39686384  5 train mse: 1.253549  5 train mse: 1.2133304  5 train mse: 1.2502793  5 train mse: 1.2864689  5 train mse: 1.2623999  5 train mse: 1.2609704  5 train mse: 1.2451407  train mse: 1.2468597 5 train mse: 1.242531  1.2490051  train mse: 1.2548016  5 train mse: 1.2463188  5 train mse: 1.2525258  5 train mse: 1.2545514 1.2621404  train mse: 1.259994 train mse: 1.2675833  5 train mse: 1.2632648 train mse: 1.262109 5 train mse: 1.2562171 1.2598401  5 train mse: 1.2578926 \n",
      "Epoch: 6 train mse: 1.2776903 \t vaild mse: 1.388673942  6 train mse: 1.2070853  6 train mse: 1.1944683   6 train mse: 1.1588024  6 train mse: 1.2010375  6 train mse: 1.2019871  6 train mse: 1.2128452  6 train mse: 1.2468143  6 train mse: 1.2513989  6 train mse: 1.2612567   6 train mse: 1.2656102 \n",
      "Epoch: 7 train mse: 1.3096911  7 train mse: 1.214677  7 train mse: 1.3099078  train mse: 1.2743934  7 train mse: 1.2676969  7 train mse: 1.3631381  7 train mse: 1.3499454  7 train mse: 1.3551348  7 train mse: 1.3291616  7 train mse: 1.320612 1.3277928   7 train mse: 1.3168164  7 train mse: 1.3057922  7 train mse: 1.3052791  7 train mse: 1.3059199 \t vaild mse: 1.3915308\n",
      "Epoch: 8 train mse: 1.2821236 \t vaild mse: 1.3937663: 1.206486 1.2387438  train mse: 1.2671325  1.2789338  train mse: 1.280602  8 train mse: 1.287633 1.2920654 \n",
      "Epoch: 9 train mse: 1.2346231 \t vaild mse: 1.38809918  train mse: 1.2585137  1.2380154  9 train mse: 1.2293661  train mse: 1.2329856 9 train mse: 1.2350465  9 train mse: 1.2419052 9 train mse: 1.2401264  9 train mse: 1.2404495  9 train mse: 1.2427717  9 train mse: 1.2289653 9 train mse: 1.2282118 \n",
      "Epoch: 10 train mse: 1.2443153 \t vaild mse: 1.40493303  10 train mse: 1.1441115  10 train mse: 1.2335738  10 train mse: 1.2523855  10 train mse: 1.3062273  10 train mse: 1.304615  10 train mse: 1.2923151  10 train mse: 1.2726707  10 train mse: 1.2659302 10 train mse: 1.2626561  10 train mse: 1.2596968  10 train mse: 1.2608109  10 train mse: 1.2736746  10 train mse: 1.2640553  10 train mse: 1.2621905  1.257586  10 train mse: 1.241647  10 train mse: 1.2455446 \n",
      "Epoch: 11 train mse: 1.2486937 \t vaild mse: 1.3904439343  11 train mse: 1.252276 1.2650981  11 train mse: 1.2371223  11 train mse: 1.254386  11 train mse: 1.261106  11 train mse: 1.2561888  11 train mse: 1.246628  11 train mse: 1.2631333  11 train mse: 1.2568274  11 train mse: 1.247585  11 train mse: 1.2394222  11 train mse: 1.2542926  11 train mse: 1.2587304  11 train mse: 1.2582518  11 train mse: 1.2603109 11 train mse: 1.2604649  11 train mse: 1.25593  11 train mse: 1.2570376 1.254254  11 train mse: 1.2479311  11 train mse: 1.2514384 \n",
      "Epoch: 12 train mse: 1.2735974 \t vaild mse: 1.390928706  12 train mse: 1.1751847  12 train mse: 1.1806616 1.2486578 1.2820306  12 train mse: 1.2793987  1.2946898  12 train mse: 1.2891219  12 train mse: 1.2813557  12 train mse: 1.2756493  12 train mse: 1.262854  12 train mse: 1.2683679  12 train mse: 1.2675583  12 train mse: 1.2684287  12 train mse: 1.2707139    12 train mse: 1.2683794  12 train mse: 1.269151 \n",
      "Epoch: 13 train mse: 1.2579149 \t vaild mse: 1.390967453  13 train mse: 1.2751421  13 train mse: 1.2429969  13 train mse: 1.2550824 train mse: 1.2668481  13 train mse: 1.2701428  13 train mse: 1.268617  13 train mse: 1.2702558  13 train mse: 1.2698362  13 train mse: 1.2657003  13 train mse: 1.2688838  13 train mse: 1.266254  13 train mse: 1.2665423  13 train mse: 1.2577591 \n",
      "Epoch: 14 train mse: 1.2663119 \t vaild mse: 1.38573076  1.3274946 1.3127884  14 train mse: 1.2793573  14 train mse: 1.2788215  14 train mse: 1.2768109  14 train mse: 1.267213  14 train mse: 1.26604  14 train mse: 1.2674843  14 train mse: 1.2587317  14 train mse: 1.2568552  14 train mse: 1.2596897  14 train mse: 1.2636976  14 train mse: 1.265906 \n",
      "Epoch: 15 train mse: 1.2371681 \t vaild mse: 1.385716218   15 train mse: 1.2413467  15 train mse: 1.2381862  15 train mse: 1.2358729  15 train mse: 1.2511889  15 train mse: 1.2491966   15 train mse: 1.2628841  1.2578124  15 train mse: 1.2615124  15 train mse: 1.2567238  15 train mse: 1.2374532  15 train mse: 1.2365502  15 train mse: 1.2335589  15 train mse: 1.2325145  15 train mse: 1.2326385 \n",
      "Epoch: 16 train mse: 1.2757298 \t16 train mse: 1.1974006 1.2767037  16 train mse: 1.2584275 1.2544533 1.2590517  16 train mse: 1.2617041  16 train mse: 1.2526245  16 train mse: 1.2467467  16 train mse: 1.2463086  16 train mse: 1.2416967  16 train mse: 1.2403573  16 train mse: 1.2391596  16 train mse: 1.239551  16 train mse: 1.2488505  16 train mse: 1.2592589  16 train mse: 1.2628314  16 train mse: 1.2644154 1.266985  16 train mse: 1.2727789  16 train mse: 1.2744054  vaild mse: 1.3853129\n",
      "Epoch: 17 train mse: 1.2336843 \t vaild mse: 1.4069881752 train mse: 1.1737019  17 train mse: 1.2044839  17 train mse: 1.1816033  1.2371982  17 train mse: 1.2185762  17 train mse: 1.2146723  17 train mse: 1.2135044  17 train mse: 1.224351  17 train mse: 1.2289537  17 train mse: 1.2325413  17 train mse: 1.234161  17 train mse: 1.2318515  17 train mse: 1.2355555 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 train mse: 1.2518859 \t vaild mse: 1.38615576524  18 train mse: 1.1607409  18 train mse: 1.1724759  18 train mse: 1.1761842  18 train mse: 1.2166624  18 train mse: 1.2128958  18 train mse: 1.2225771  18 train mse: 1.237911 train mse: 1.2418007  18 train mse: 1.2438759  18 train mse: 1.2461904  18 train mse: 1.239605  18 train mse: 1.2401295  18 train mse: 1.2464494  18 train mse: 1.2452071  18 train mse: 1.2493552 \n",
      "Epoch: 19 train mse: 1.2550474 \t vaild mse: 1.385635408  19 train mse: 1.2749732  19 train mse: 1.2505207  19 train mse: 1.2690206  19 train mse: 1.2694433 19 train mse: 1.2776837  train mse: 1.2728988  19 train mse: 1.2650878 1.2454035  train mse: 1.2561934  19 train mse: 1.255018  19 train mse: 1.2574301 \n",
      "Epoch: 20 train mse: 1.2366562  20 train mse: 1.240016  20 train mse: 1.2388598  20 train mse: 1.1829149  20 train mse: 1.1954257  1.2142439  20 train mse: 1.2242236  20 train mse: 1.2292529  20 train mse: 1.2295483  20 train mse: 1.2284087  20 train mse: 1.2285042  20 train mse: 1.228737  20 train mse: 1.2323282  20 train mse: 1.2341098  20 train mse: 1.23279 \t vaild mse: 1.3890855\n",
      "Epoch: 21 train mse: 1.285487 \t vaild mse: 1.3842337842  21 train mse: 1.3153245  21 train mse: 1.3003643  21 train mse: 1.2856771  train mse: 1.3060454  21 train mse: 1.2444252  21 train mse: 1.2436792  21 train mse: 1.2467365  21 train mse: 1.2498416 1.2493837  21 train mse: 1.2348591  21 train mse: 1.2314003  21 train mse: 1.2653642  21 train mse: 1.2786214 \n",
      "Epoch: 22 train mse: 1.231708 \t vaild mse: 1.3971624238   22 train mse: 1.2785478 1.2738025 train mse: 1.2529327  1.2487969  22 train mse: 1.2394586 22 train mse: 1.2485127  22 train mse: 1.2506841  22 train mse: 1.262577  22 train mse: 1.2491528  train mse: 1.2504885  22 train mse: 1.2418325 1.2316866 \n",
      "Epoch: 23 train mse: 1.2592108 \t vaild mse: 1.387640438  23 train mse: 1.3227195  23 train mse: 1.2681385  23 train mse: 1.2573104  23 train mse: 1.3050604 1.2858294  23 train mse: 1.2866409  23 train mse: 1.2669511  23 train mse: 1.268523  23 train mse: 1.2516577  23 train mse: 1.2581725  23 train mse: 1.2573988  1.2567492  23 train mse: 1.255285  23 train mse: 1.2569432 \n",
      "Epoch: 24 train mse: 1.2740766 \t vaild mse: 1.384029682 24 train mse: 1.3347212  24 train mse: 1.3138471 1.3130432  24 train mse: 1.3049719  24 train mse: 1.3045126  24 train mse: 1.2928636  24 train mse: 1.2783364  24 train mse: 1.2832441  24 train mse: 1.2715873  24 train mse: 1.2803143  24 train mse: 1.2780656  train mse: 1.2809446  24 train mse: 1.2710116 \n",
      "Epoch: 25 train mse: 1.272739 \t vaild mse: 1.38540423529  25 train mse: 1.2865797  25 train mse: 1.2887992  1.30411  25 train mse: 1.2846738  25 train mse: 1.2799611  25 train mse: 1.2903588  25 train mse: 1.2849643  25 train mse: 1.2886462  25 train mse: 1.2919908  25 train mse: 1.2966381  25 train mse: 1.2869763  25 train mse: 1.285405  25 train mse: 1.2740009  25 train mse: 1.275643  25 train mse: 1.2766061  25 train mse: 1.2729613 \n",
      "Epoch: 26 train mse: 1.2680519 \t vaild mse: 1.384861548  26 train mse: 1.237545  26 train mse: 1.1480756  26 train mse: 1.2047088  26 train mse: 1.2051028  26 train mse: 1.2134508  26 train mse: 1.2191336  26 train mse: 1.2469224  train mse: 1.2580197  26 train mse: 1.2664388  26 train mse: 1.2579659  26 train mse: 1.2677873 \n",
      "Epoch: 27 train mse: 1.2711606 \t vaild mse: 1.396255654  27 train mse: 1.3579583  27 train mse: 1.2535574  train mse: 1.3111045  27 train mse: 1.2811612  27 train mse: 1.2734574  27 train mse: 1.2713252  27 train mse: 1.2710296 \n",
      "Epoch: 28 train mse: 1.2419251  28 train mse: 1.2206328  28 train mse: 1.2128094  28 train mse: 1.2575259 train mse: 1.2603968  1.263139  28 train mse: 1.2432353  train mse: 1.2445457 \t vaild mse: 1.3929112\n",
      "Epoch: 29 train mse: 1.25646 \t vaild mse: 1.3854742425   29 train mse: 1.2596067  29 train mse: 1.2593477  29 train mse: 1.2468119  29 train mse: 1.2403768  29 train mse: 1.2578897  29 train mse: 1.2438688  29 train mse: 1.2508378   29 train mse: 1.2413375  29 train mse: 1.2518617  29 train mse: 1.2601253 train mse: 1.259904  1.2617221  29 train mse: 1.2615391  29 train mse: 1.2611123  train mse: 1.2509729  29 train mse: 1.2492269  29 train mse: 1.251801 \n",
      "Epoch: 30 train mse: 1.2302551 \t vaild mse: 1.384679315  30 train mse: 1.2424237  train mse: 1.2673565  30 train mse: 1.2288585  30 train mse: 1.2304869  30 train mse: 1.2186804 1.2163419  30 train mse: 1.2144858  30 train mse: 1.2051213 \n",
      "Epoch: 31 train mse: 1.2806662 \t vaild mse: 1.383895508 1.2166694  31 train mse: 1.2665035  31 train mse: 1.2780751  31 train mse: 1.2808349 1.2897891  31 train mse: 1.286184  31 train mse: 1.2894082  train mse: 1.2796882 1.2772253 \n",
      "Epoch: 32 train mse: 1.2496618 \t vaild mse: 1.383929375  32 train mse: 1.2568762 1.2951608 1.2780657 32 train mse: 1.2651831  32 train mse: 1.2612954  32 train mse: 1.2585028  32 train mse: 1.2555327  32 train mse: 1.2548417  32 train mse: 1.252335  32 train mse: 1.2420862  32 train mse: 1.2501028  32 train mse: 1.2493857  32 train mse: 1.2458687  32 train mse: 1.2380469  32 train mse: 1.2387431  32 train mse: 1.2317555  32 train mse: 1.2388911  32 train mse: 1.2373911  32 train mse: 1.2467451  32 train mse: 1.253919  32 train mse: 1.2467611  32 train mse: 1.2473468   32 train mse: 1.247103  32 train mse: 1.2509775 \n",
      "Epoch: 33 train mse: 1.2273108 \t vaild mse: 1.3874601  33 train mse: 1.216439 33 train mse: 1.2275518  33 train mse: 1.1967334  33 train mse: 1.200373  33 train mse: 1.202467  33 train mse: 1.1987268  33 train mse: 1.2300793  33 train mse: 1.235337  33 train mse: 1.2329125  33 train mse: 1.228367  33 train mse: 1.2267413  33 train mse: 1.2242326  1.2213922  33 train mse: 1.2239294 \n",
      "Epoch: 34 train mse: 1.2255839 \t34 train mse: 1.0752568  train mse: 1.2578297  train mse: 1.2463286  34 train mse: 1.2374009  34 train mse: 1.2088803  34 train mse: 1.2121055 1.2048774 1.1975532 train mse: 1.2110488  1.2189845  34 train mse: 1.2171103  vaild mse: 1.3854975\n",
      "Epoch: 35 train mse: 1.27426 7  35 train mse: 1.2889727 train mse: 1.2781067 35 train mse: 1.2617596  35 train mse: 1.2709574  35 train mse: 1.2675545  35 train mse: 1.2657924  35 train mse: 1.269614 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4b89e2ea3feb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\rEpoch:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train mse:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0my_vaild_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\THE FOOL\\Anaconda3\\envs\\py3_tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    439\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m           kwargs={\"name\": name})\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\THE FOOL\\Anaconda3\\envs\\py3_tf2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1915\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\THE FOOL\\Anaconda3\\envs\\py3_tf2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1922\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[0;32m   1923\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1924\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1925\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\THE FOOL\\Anaconda3\\envs\\py3_tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[1;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\THE FOOL\\Anaconda3\\envs\\py3_tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[1;34m(self, delta, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    779\u001b[0m       assign_add_op = gen_resource_variable_ops.assign_add_variable_op(\n\u001b[0;32m    780\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    782\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_add_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\THE FOOL\\Anaconda3\\envs\\py3_tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_add_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;34m\"AssignAddVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         value)\n\u001b[0m\u001b[0;32m     52\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  1.batch 遍历数据集 metric\n",
    "#     1.1 自动求导\n",
    "#  2.epoch结束 验证数据集 metric\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(x_train_scaled) // batch_size\n",
    "optimizer = keras.optimizers.SGD()\n",
    "metric = keras.metrics.MeanSquaredError()\n",
    "\n",
    "def random_batch(x, y, batch_size=32):\n",
    "    idx = np.random.randint(0, len(x), size= batch_size)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu,input_shape=x_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    metric.reset_states()\n",
    "    for step in range(steps_per_epoch):\n",
    "        x_batch, y_batch = random_batch(x_train_scaled, y_train, batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = tf.reduce_mean(keras.losses.mean_squared_error(y_batch, y_pred))\n",
    "            metric(y_batch, y_pred)\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        grads_and_vars = zip(grads, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "        print(\"\\rEpoch:\", epoch, \"train mse:\", metric.result().numpy(), end=\" \")\n",
    "    y_vaild_pred = model(x_valid_scaled)\n",
    "    vaild_loss = tf.reduce_mean(keras.losses.mean_squared_error(y_valid, y_vaild_pred))\n",
    "    print(\"\\t\", \"vaild mse:\", vaild_loss.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
